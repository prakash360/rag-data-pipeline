{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbeb6b4c-0463-4d32-ad8e-9cce9013c693",
   "metadata": {},
   "source": [
    "# RAG Data Pipeline \n",
    "\n",
    "## Key goals of this notebook: \n",
    "- Read PDF files\n",
    "- Convert into splits\n",
    "- Create a Vector Database and Store Embeddings\n",
    "- Query the Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d500388-f9b4-4a32-ae5f-64fa072330a3",
   "metadata": {},
   "source": [
    "## Install and load some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c19b4bd-9b03-4ff5-9dc5-d9a16366f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU PyPDF2 langchain tiktoken sentence_transformers langchain-community pypdf openai python-dotenv chromadb langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a0884a-8948-4cba-a238-1cc566da3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbda33e-decf-489d-aa05-8f1de15bc0d9",
   "metadata": {},
   "source": [
    "## Load OPEN AI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df20ea18-4cfe-47cf-b11b-69b8864fa6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key Loaded: True\n"
     ]
    }
   ],
   "source": [
    "# if you're starting from scartch, you might need to load openAI api key to .env file using following code..  \n",
    "# !echo \"OPENAI_API_KEY=replacemykeyhere\" >> .env\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Access the OpenAI API key from environment variables\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "print(\"OpenAI API Key Loaded:\", openai.api_key is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbcf07-48d8-43a7-82e2-fedd868dfc54",
   "metadata": {},
   "source": [
    "## Reading of PDF files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d178702-1c32-44c9-9598-1426c4bc1123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 document(s) from the PDF.\n",
      "First document content preview: Revisiting OPRO: The Limitations of Small-Scale LL\n"
     ]
    }
   ],
   "source": [
    "file_path = \"2405.10276v1.pdf\"\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Verify that the document is loaded correctly\n",
    "if docs:\n",
    "    print(f\"Loaded {len(docs)} document(s) from the PDF.\")\n",
    "    print(f\"First document content preview: {docs[0].page_content[:50]}\")\n",
    "else:\n",
    "    print(\"No documents loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52078b2a-103b-4cee-93de-c92d0a95dead",
   "metadata": {},
   "source": [
    "## Splitting of PDF files into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095e4539-1eec-4e61-a6c7-142828954f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into chunks using TokenTextSplitter\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 50\n",
    "text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d901db8-1287-44eb-877f-ceeb53782d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits: 127\n"
     ]
    }
   ],
   "source": [
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"Number of splits: {len(splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1fd0538-921e-4065-97eb-0d81951a6d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers\n",
      "Tuo Zhang∗Jinyue Yuan∗and Salman Avestimehr\n",
      "University of Southern California\n",
      "{tuozhang, jinyueyu, avestime}@usc.edu\n",
      "Abstract\n",
      "Numerous recent works aim to enhance the\n",
      "\n",
      "Chunk 2:\n",
      "Abstract\n",
      "Numerous recent works aim to enhance the\n",
      "efficacy of Large Language Models (LLMs)\n",
      "through strategic prompting. In particular, the\n",
      "Optimization by PROmpting (OPRO) approach\n",
      "provides state-of-the-art performance by lever-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first few chunks to verify\n",
    "for i, chunk in enumerate(splits[:2]):\n",
    "    print(f\"Chunk {i + 1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354f1bd-3078-4d60-afb4-ffe5f64b64d4",
   "metadata": {},
   "source": [
    "## Create a vector database and store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3004ae10-c2d7-4bfb-ad9d-fa6994a9be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f878aad-038e-4ab4-ba30-0cd2c34e814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = \"This is a test sentence.\"\n",
    "# sample_embedding = embedding.embed_documents([sample_text])\n",
    "# print(sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78a9bbc9-b8fe-4ee6-a5fd-f5d499463be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the previosuly created vector store\n",
    "persist_directory = 'docs/chroma/'\n",
    "!rm -rf ./docs/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeab841d-f2fc-46d0-902e-fbb94d26df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Document Splits:127\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Document Splits:{len(splits)}\")\n",
    "# print(\"splits::\", splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119f0f24-27bc-4d99-b8a6-193a714e2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma/'\n",
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5fa4ed-0ce9-40e8-896b-01443d59faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e2571c-9736-4ad5-b8c3-b0c4145ac707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in my vector DB: 127\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents in my vector DB: {vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13c3abd4-a442-4d19-8f02-af7bef04da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectordb.get(include=['embeddings', 'documents', 'metadatas']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b801eb7-04c1-4278-8201-8d59e1489eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"what is the motivation for OPRO?\"\n",
    "question = \"What are the key limitations of small-scale language models like LLaMa when used as optimizers for automated prompt engineering techniques like OPRO?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f88225b-d60f-4ffc-8032-53790b8719ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.similarity_search_with_score(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6836bd3e-d52c-42b4-9d6e-a6aac502e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='scorer in OPRO’s context, leading to suboptimal\\nprompt generation. As a result, due to the limited\\ninference ability, small-scale LLMs could not sup-\\nport self-optimization for prompting paradigms.\\nHuman-Crafted Elements and Their Impacts.', metadata={'page': 3, 'source': '2405.10276v1.pdf'}), 0.2309589385986328), (Document(page_content='paper, we revisit OPRO for automated prompt-\\ning with relatively small-scale LLMs, such as\\nLLaMa-2 family and Mistral 7B . Our inves-\\ntigation reveals that OPRO shows limited ef-\\nfectiveness in small-scale LLMs, with limited', metadata={'page': 0, 'source': '2405.10276v1.pdf'}), 0.24007278680801392), (Document(page_content='Abstract\\nNumerous recent works aim to enhance the\\nefficacy of Large Language Models (LLMs)\\nthrough strategic prompting. In particular, the\\nOptimization by PROmpting (OPRO) approach\\nprovides state-of-the-art performance by lever-', metadata={'page': 0, 'source': '2405.10276v1.pdf'}), 0.24748815596103668)]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a6a7ce-43e7-4275-8feb-fb9c1ff1fbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scorer in OPRO’s context, leading to suboptimal\n",
      "prompt generation. As a result, due to the limited\n",
      "inference ability, small-scale LLMs could not sup-\n",
      "port self-optimization for prompting paradigms.\n",
      "Human-Crafted Elements and Their Impacts.\n",
      "paper, we revisit OPRO for automated prompt-\n",
      "ing with relatively small-scale LLMs, such as\n",
      "LLaMa-2 family and Mistral 7B . Our inves-\n",
      "tigation reveals that OPRO shows limited ef-\n",
      "fectiveness in small-scale LLMs, with limited\n",
      "Abstract\n",
      "Numerous recent works aim to enhance the\n",
      "efficacy of Large Language Models (LLMs)\n",
      "through strategic prompting. In particular, the\n",
      "Optimization by PROmpting (OPRO) approach\n",
      "provides state-of-the-art performance by lever-\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(question,k=3)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"{result.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf4a9ab-1e00-4b91-9f65-4352a08f8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5180e3fa-d3b9-4aef-8893-33247a4ba593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the text if necessary\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# cleaned_text = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a2eca-f4a1-4e34-aa11-386d253e18d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc9436-b340-40dc-b7b9-d2a6206dc265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
